# from_scratch

Basic Neural network 
  - Feedforward
  - Backpropagation
  - ReLU, Leaky ReLU, Sigmoid, Softmax
  - Gradient Descent

Optimization Algorithms
  - SGD
  - Momentum
  - Adam

Recurrent Neural Networks
  - Sequential Data Processing
  - Backpropagation Through Time
  - Vanishing and Exploding Gradients

LSTM and GRUs

Transformer
  - Self-Attention Mechanism
  - Multi-Head Attention
  - Positional Encoding
  - Feedforward Network in Transformers
  - Encoder
  - Decoder
